# main.py

import tensorflow as tf
import torch
import os
from datetime import date
from concurrent.futures import ThreadPoolExecutor
import cv2  # ‡πÑ‡∏°‡πà‡∏•‡∏∑‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ import cv2

from face_detection import detect_faces
from face_encoding import encode_face
from database import get_student_encodings, recognize_face, mark_attendance

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU
print("TensorFlow CUDA Available:", tf.config.list_physical_devices('GPU'))
print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"Number of GPUs: {torch.cuda.device_count()}")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ TensorFlow ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ GPU 0 (‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠‡∏´‡∏•‡∏±‡∏Å)
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.set_visible_devices(physical_devices[0], 'GPU')  # ‡πÉ‡∏ä‡πâ GPU 0
else:
    print("GPU 0 not found, defaulting to CPU.")

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Running on: {device.upper()}")

os.makedirs("result", exist_ok=True)

video_path = "videos/VideoCut_5_min/output_segment_1.mp4"
cap = cv2.VideoCapture(video_path)

if not cap.isOpened():
    print("Can't Open Video")
    exit()

frame_count = 0
student_encodings = get_student_encodings()  # ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß

def process_face(face, frame):
    encoding = encode_face(frame, face)
    if encoding is None:
        print("Skipping invalid face")
        return

    print("Face Encoding Complete")
    recognized_id = recognize_face(encoding, student_encodings)

    if recognized_id:
        print(f"Recognized Student ID: {recognized_id}")
        mark_attendance(recognized_id, date.today())
    else:
        print("Face Not Recognized")

with ThreadPoolExecutor() as executor:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("Video End!")
            break

        frame_count += 1

        if frame_count % 5 != 0:
            continue

        print(f"Processing Frame {frame_count}")

        frame_resized = cv2.resize(frame, (640, 360))
        faces = detect_faces(frame_resized)

        print(f"Detected {len(faces)} Faces")
        for face in faces:
            executor.submit(process_face, face, frame_resized.copy())

cap.release()
cv2.destroyAllWindows()
print("Program End...")
-----------------------------------------------------------------------------
# face_encoding.py

import cv2
import numpy as np
from deepface import DeepFace
import os
import tensorflow as tf

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ GPU 0 ‡πÉ‡∏ô TensorFlow
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.set_visible_devices(physical_devices[0], 'GPU')  # ‡πÉ‡∏ä‡πâ GPU 0
else:
    print("GPU 0 not found, defaulting to CPU.")

arcface_model = DeepFace.build_model("ArcFace")

os.makedirs("result", exist_ok=True)

def encode_face(frame, face):
    x1, y1, x2, y2 = face

    if x1 >= x2 or y1 >= y2:
        print(f"Invalid face coordinates: {face}")
        return None

    face_crop = frame[y1:y2, x1:x2]

    if face_crop.shape[0] == 0 or face_crop.shape[1] == 0:
        print("Invalid face crop: Zero dimension")
        return None

    # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    face_resized = cv2.resize(face_crop, (224, 300))  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå result/
    #filename = f"images/resultFromFRAS/GPU8-f5-d0.2-e640x640/face_{x1}_{y1}.jpg"
    #cv2.imwrite(filename, face_resized)  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß
    #print(f"Saved face to: {filename}")

    representation = DeepFace.represent(face_resized, model_name="ArcFace", enforce_detection=False)

    if not representation:
        print("No embedding generated")
        return None

    return np.array(representation[0]['embedding']).flatten()
-----------------------------------------------------------------------------
# face_detection.py

import cv2
import numpy as np
import os

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• DNN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
model_file = "models/opencv_face_detector_uint8.pb"
config_file = "models/opencv_face_detector.pbtxt"

net = cv2.dnn.readNetFromTensorflow(model_file, config_file)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ GPU 0 (‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠‡∏´‡∏•‡∏±‡∏Å) ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
if cv2.cuda.getCudaEnabledDeviceCount() > 0:
    cv2.cuda.setDevice(0)  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å GPU 0
    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)
else:
    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_DEFAULT)
    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)

def detect_faces(frame):
    h, w = frame.shape[:2]
    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),
                                 [104, 117, 123], False, False)
    net.setInput(blob)
    detections = net.forward()

    faces = []
    for i in range(detections.shape[2]):
        confidence = detections[0, 0, i, 2]
        if confidence > 0.2:
            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
            x1, y1, x2, y2 = box.astype("int")
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(w - 1, x2), min(h - 1, y2)
            faces.append((x1, y1, x2, y2))

    return faces
-----------------------------------------------------------------------------
#database.py

import pyodbc
import numpy as np
from config import DATABASE_CONFIG
from scipy.spatial import distance
from datetime import date

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
conn = pyodbc.connect(f"DRIVER={DATABASE_CONFIG['DRIVER']};"
                      f"SERVER={DATABASE_CONFIG['SERVER']};"
                      f"DATABASE={DATABASE_CONFIG['DATABASE']};"
                      f"UID={DATABASE_CONFIG['UID']};"
                      f"PWD={DATABASE_CONFIG['PWD']};"
                      "Encrypt=yes;"
                      "TrustServerCertificate=no;"
                      "Connection Timeout=30;")
cursor = conn.cursor()

# üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Students
def save_encoding(student_id, encoding):
    encoding_bytes = encoding.tobytes()
    query = "UPDATE Students SET face_embedding = ? WHERE student_id = ?"
    cursor.execute(query, (encoding_bytes, student_id))
    conn.commit()

# üîπ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Face Encoding ‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
def get_student_encodings():
    query = "SELECT student_id, face_embedding FROM Students WHERE face_embedding IS NOT NULL"
    print(f"Executing SQL Query: {query}")  # Debug log
    
    cursor.execute(query)
    students = cursor.fetchall()
    
    print(f"Found {len(students)} students with encodings")  # Debug log
    
    student_encodings = {}
    for student_id, encoding_bytes in students:
        encoding = np.frombuffer(encoding_bytes, dtype=np.float64)
        student_encodings[student_id] = encoding
    
    return student_encodings

# üîπ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
def recognize_face(encoding, student_encodings):
    for student_id, db_encoding in student_encodings.items():
        dist = distance.euclidean(encoding, db_encoding)
        print(f"Comparing {student_id}: Distance = {dist}")  # Debug log
        if dist < 0.6:  # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
            return student_id
    return None

# üîπ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏ô Attendance
def mark_attendance(student_id, check_date):
    query = """
    INSERT INTO Attendance (course_id, course_name, student_id, first_name, last_name, date_check, status_student)
    SELECT e.course_id, c.course_name, s.student_id, s.student_name, '', ?, '00'
    FROM Students s
    JOIN Enrollment e ON s.student_id = e.student_id
    JOIN Courses c ON e.course_id = c.course_id
    WHERE s.student_id = ?
    """
    cursor.execute(query, check_date, student_id)
    conn.commit()
    print(f"Attendance Recorded for Student {student_id} on {check_date}")
-----------------------------------------------------------------------------
#Create_FE.py (create face embedding refferance)


import cv2
import numpy as np
import pyodbc
import random
import json  # ‡πÉ‡∏ä‡πâ json ‡πÅ‡∏ó‡∏ô pickle
from insightface.app import FaceAnalysis  # ‡πÉ‡∏ä‡πâ ArcFace
from config import DATABASE_CONFIG
from PIL import Image
import io

# üîπ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Azure SQL
conn = pyodbc.connect(f"DRIVER={DATABASE_CONFIG['DRIVER']};"
                      f"SERVER={DATABASE_CONFIG['SERVER']};"
                      f"DATABASE={DATABASE_CONFIG['DATABASE']};"
                      f"UID={DATABASE_CONFIG['UID']};"
                      f"PWD={DATABASE_CONFIG['PWD']};"
                      "Encrypt=yes;"
                      "TrustServerCertificate=no;"
                      "Connection Timeout=30;")
cursor = conn.cursor()

# üîπ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ArcFace (‡πÉ‡∏ä‡πâ GPU ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
face_app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
face_app.prepare(ctx_id=0)  # ‡πÉ‡∏ä‡πâ GPU ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ

# üîπ ‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
video_path = "videos/REF Face/65009974.mp4"
cap = cv2.VideoCapture(video_path)

if not cap.isOpened():
    print("Can't Open Video")
    exit()

# üîπ ‡∏Å‡∏£‡∏≠‡∏Å‡∏£‡∏´‡∏±‡∏™‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤
student_id = input("Insert StudentID: ")

# üîπ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
selected_frames = sorted([int(i * frame_count / 10) for i in range(10)])  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢

face_embeddings = []

# üîπ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
def save_original_frame(frame, image_count):
    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB
    pil_img = Image.fromarray(img)
    pil_img.save(f"images/results/original_frame_{image_count}.png")  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö

# üîπ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö 10 ‡πÄ‡∏ü‡∏£‡∏°
image_count = 1  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà 1
processed_frames = 0  # ‡πÉ‡∏ä‡πâ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ü‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à

for idx, frame_no in enumerate(selected_frames):
    while processed_frames < 10:
        try:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)  # ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÄ‡∏ü‡∏£‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
            ret, frame = cap.read()

            if not ret:
                print(f"Skipped Frame {frame_no}: Failed to read frame, trying next.")
                frame_no += 1  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                if frame_no >= frame_count:
                    print("Reached end of video.")
                    break
                continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏ü‡∏£‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ

            print(f"Processing Frame {frame_no}")

            # üîπ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            faces_detected = face_app.get(frame)
            if len(faces_detected) == 0:
                print(f"Skipped Frame {frame_no}: No face detected.")
                frame_no += 1  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                if frame_no >= frame_count:
                    print("Reached end of video.")
                    break
                continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏ü‡∏£‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤

            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
            face_embedding = faces_detected[0].embedding
            face_embeddings.append(face_embedding)

            # üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            save_original_frame(frame, image_count)
            image_count += 1  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏†‡∏≤‡∏û‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à

            processed_frames += 1  # ‡∏ô‡∏±‡∏ö‡πÄ‡∏ü‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à

            # üîπ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏™‡∏á
            for _ in range(10):  
                bright_embedding = face_embedding + np.random.normal(0.1, 0.1, face_embedding.shape)
                face_embeddings.append(bright_embedding)  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å embedding ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏™‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°

            # üîπ ‡∏•‡∏î‡πÅ‡∏™‡∏á
            for _ in range(10):  
                dark_embedding = face_embedding - np.random.normal(0.1, 0.1, face_embedding.shape)
                face_embeddings.append(dark_embedding)  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å embedding ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏™‡∏á‡∏•‡∏î

            # üîπ ‡πÄ‡∏ö‡∏•‡∏≠‡∏†‡∏≤‡∏û
            for _ in range(10):  
                blur_embedding = np.copy(face_embedding)  # ‡πÄ‡∏ö‡∏•‡∏≠‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á embedding ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ
                blur_embedding += np.random.normal(0, 0.1, blur_embedding.shape)
                face_embeddings.append(blur_embedding)  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å embedding ‡∏ó‡∏µ‡πà‡πÄ‡∏ö‡∏•‡∏≠

            # üîπ ‡∏†‡∏≤‡∏û‡∏´‡∏≤‡∏¢
            for _ in range(10):  
                missing_embedding = face_embedding + np.random.normal(0, 0.15, face_embedding.shape)  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏¢‡∏ö‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                face_embeddings.append(missing_embedding)  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å embedding ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢

            # üîπ ‡πÄ‡∏û‡∏¥‡πà‡∏° Noise ‡πÅ‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            for _ in range(50):  
                modified_embedding = face_embedding + np.random.normal(0, 0.1, face_embedding.shape)
                face_embeddings.append(modified_embedding)  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å embedding ‡∏ó‡∏µ‡πà‡∏°‡∏µ noise

            # üîπ ‡πÅ‡∏õ‡∏•‡∏á `face_embeddings` ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô JSON
            face_embeddings_json = json.dumps([embedding.tolist() for embedding in face_embeddings])

            # üîπ ‡πÅ‡∏õ‡∏•‡∏á JSON ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ö‡∏ï‡πå (binary) ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            face_embeddings_binary = face_embeddings_json.encode('utf-8')

            # üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å face_embeddings ‡∏•‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            cursor.execute("""
                UPDATE Students
                SET face_embedding = ?
                WHERE student_id = ?
            """, (face_embeddings_binary, student_id))
            conn.commit()

            print(f"Student {student_id} has face embeddings stored")

            break  # ‡∏´‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ü‡∏£‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡∏•‡∏π‡∏õ‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏ü‡∏£‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

        except Exception as e:
            print(f"Error processing frame {frame_no}: {e}")
            frame_no += 1  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
            if frame_no >= frame_count:
                print("Reached end of video.")
                break
            continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏ü‡∏£‡∏°‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

# üîπ ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô
print(f"10 original images saved to the face folder.")
print(f"100 embeddings (including noise, brightness, etc.) saved.")
print(f"Student {student_id} has face embeddings stored.")

cap.release()
conn.close()
-----------------------------------------------------------------------------
#config.py

DATABASE_CONFIG = {
    "DRIVER": "{ODBC Driver 18 for SQL Server}",
    "SERVER": "fras.database.windows.net",
    "DATABASE": "FRAS",
    "UID": "cpe495",
    "PWD": "Mean2003."
}
FACE_RECOGNITION_THRESHOLD = 0.6
-----------------------------------------------------------------------------
#CapIpCamTest.py (Cap Video From RTSP of IP Camera)

import cv2
import time
from datetime import datetime

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î URL RTSP
USERNAME = "MeanSC11"
PASSWORD = "Mean2003."
CAMERA_IP = "192.168.1.100"
RTSP_URL = "rtsp://MeanSC11:Mean2003.@192.168.1.100:554/stream1"

# ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
start_hour = 11
start_minute = 45

# ‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏•‡πâ‡∏≠‡∏á
cap = cv2.VideoCapture(RTSP_URL)

if not cap.isOpened():
    print("Error: Unable to connect to the camera.")
    exit()
else:
    print("Connected to Camera!!")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡∏†‡∏≤‡∏û‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
ret, frame = cap.read()
if not ret:
    print("Error: Failed to grab frame from camera.")
    cap.release()
    exit()

# ‡∏£‡∏≠‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°
while True:
    now = datetime.now()
    if now.hour == start_hour and now.minute == start_minute:
        print(f"Starting recording at {start_hour}:{start_minute}")
        break
    time.sleep(1)  # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏∏‡∏Å 1 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ß‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏¢‡∏±‡∏á‡∏™‡πà‡∏á‡∏†‡∏≤‡∏û‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà
ret, frame = cap.read()
if not ret:
    print("Error: Failed to grab frame from camera.")
    cap.release()
    exit()

# ‡∏î‡∏∂‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

# ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
if frame_width == 0 or frame_height == 0:
    frame_width, frame_height = 1920, 1080  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì

print(f"Connected to camera. Resolution: {frame_width}x{frame_height}")
print(f"FPS from camera: {fps}")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
fourcc = cv2.VideoWriter_fourcc(*"avc1")  # ‡πÉ‡∏ä‡πâ mp4v ‡∏ñ‡πâ‡∏≤ H264 ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
out = cv2.VideoWriter("full_record.mp4", fourcc, fps, (frame_width, frame_height))

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ (‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å 45 ‡∏ô‡∏≤‡∏ó‡∏µ)
record_time = 45 * 60  # 45 ‡∏ô‡∏≤‡∏ó‡∏µ (2700 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
start_time = time.time()
result_time = record_time/60

print(f"Recording video for {result_time} minutes...")

while cap.isOpened():
    ret, frame = cap.read()
    
    if not ret:
        print("Error: Failed to grab frame.")
        break

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏±‡∏ö‡∏ñ‡∏≠‡∏¢‡∏´‡∏•‡∏±‡∏á
    elapsed_time = time.time() - start_time
    remaining_time = max(0, record_time - elapsed_time)
    minutes = int(remaining_time // 60)
    seconds = int(remaining_time % 60)
    countdown_text = f"Time Left: {minutes:02}:{seconds:02}"

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏±‡∏ö‡∏ñ‡∏≠‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏•‡∏á‡∏ö‡∏ô‡πÄ‡∏ü‡∏£‡∏°
    font = cv2.FONT_HERSHEY_SIMPLEX
    cv2.putText(frame, countdown_text, (50, 50), font, 1, (0, 255, 0), 2, cv2.LINE_AA)

    out.write(frame)

    if remaining_time <= 0:
        break

cap.release()
out.release()
print("Recording completed. Video saved as 'full_record.mp4'.")

# *** ‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 30 ‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏•‡∏∞ 15 ‡∏ô‡∏≤‡∏ó‡∏µ ***
print("Splitting video into 30-minute and 15-minute parts...")

# ‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à
cap = cv2.VideoCapture("full_record.mp4")

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ü‡∏£‡∏°
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
fps = cap.get(cv2.CAP_PROP_FPS)
frame_30min = int(fps * 30 * 60)
frame_15min = total_frames - frame_30min

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏±‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
def split_video(start_frame, end_frame, output_name):
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
    out = cv2.VideoWriter(output_name, fourcc, fps, (frame_width, frame_height))
    for _ in range(start_frame, end_frame):
        ret, frame = cap.read()
        if not ret:
            break
        out.write(frame)
    out.release()

# ‡∏ï‡∏±‡∏î‡∏Ñ‡∏•‡∏¥‡∏õ
split_video(0, frame_30min, "part1_30min.mp4")
split_video(frame_30min, total_frames, "part2_15min.mp4")

cap.release()
out.release()
print("Splitting completed: 'part1_30min.mp4' and 'part2_15min.mp4'.")
-----------------------------------------------------------------------------
